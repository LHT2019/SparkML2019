package spark.ml
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
import org.apache.spark.ml.{Pipeline, PipelineStage}
import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, VectorAssembler}
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.ml.linalg.Vectors
import scala.collection.mutable.ListBuffer
object newtrainLR {
  def main(args: Array[String]) {
    val spark = SparkSession.builder().appName("LR-Predict").getOrCreate()
    //数据读入
    val trainPath = "../train_with_hour.csv"
    val testPath = "../test_with_hour.csv"
    val trainDF = spark.read.format("csv").option("header", "true").load(trainPath)
    val testDF: DataFrame = spark.read.format("csv").option("header", "true").load(testPath)

    val newTrainDF: DataFrame = trainDF.drop("_c0", "Unnamed: 0", "time", "city", "app_paid").withColumn("flag", lit(1))
    val newTestDF = testDF.drop("_c0", "Unnamed: 0", "time", "city").withColumn("click", lit(3)).withColumn("flag", lit(2))
    //合并train、test，一起做one-hot编码
    val allDF = newTrainDF.union(newTestDF)
    //获取列名array
    val colNameDF = allDF.drop("flag", "click")
    // 要进行OneHotEncoder编码的字段
    val categoricalColumns = colNameDF.columns
    //采用Pileline方式处理机器学习流程
    val stagesArray = new ListBuffer[PipelineStage]()
    for (cate <- categoricalColumns) {
      //使用StringIndexer 建立类别索引
      val indexer = new StringIndexer().setInputCol(cate).setOutputCol(s"${cate}Index")
      // 使用OneHotEncoder将分类变量转换为二进制稀疏向量
      val encoder = new OneHotEncoder().setInputCol(indexer.getOutputCol).setOutputCol(s"${cate}classVec")
      stagesArray.append(indexer, encoder)
    }
    val assemblerInputs = categoricalColumns.map(_ + "classVec")
    // 使用VectorAssembler将所有特征转换为一个向量
    val assembler = new VectorAssembler().setInputCols(assemblerInputs).setOutputCol("features")
    //使用pipeline批处理
    val pipeline = new Pipeline()
    pipeline.setStages(stagesArray.toArray)
    val pipelineModel = pipeline.fit(allDF)
    val dataset = pipelineModel.transform(allDF)
    val newDF = dataset.select("click", "features", "flag")
    //拆分train、test
    val processedTrain = newDF.filter(col("flag") === 1).drop("flag")
    val processedTest = newDF.filter(col("flag") === 2).drop("click", "flag")
    //处理label列
    val indexer2Click = new StringIndexer().setInputCol("click").setOutputCol("ctr")
    val finalTrainDF = indexer2Click.fit(processedTrain).transform(processedTrain).drop("click")
    //随机分割测试集和训练集数据
    val Array(trainingDF, testDF) = finalTrainDF.randomSplit(Array(0.7, 0.3), seed = 1)
    println(s"trainingDF size=${trainingDF.count()},testDF size=${testDF.count()}")
    val lrModel = new LogisticRegression().setLabelCol("ctr").setFeaturesCol("features").setMaxIter(10000).setThreshold(0.5).setRegParam(0.15).fit(trainingDF)
    val predictions = lrModel.transform(testDF).select($"ctr".as("label"), "features", "rawPrediction", "probability", "prediction")
    //使用BinaryClassificationEvaluator来评价我们的模型
    val evaluator = new BinaryClassificationEvaluator()
    evaluator.setMetricName("areaUnderROC")
    val auc = evaluator.evaluate(predictions)
    val newprediction: DataFrame = lrModel.transform(processedTest).select("probability")
    //取出预测为1的probability
    val reseult2 = newprediction.map(line => {
      val dense = line.get(line.fieldIndex("probability")).asInstanceOf[org.apache.spark.ml.linalg.DenseVector]
      val y = dense(1).toString(y)
    }).toDF("pro2ture")
    reseult2.repartition(1).write.text("../firstLrResultStr")
  }
}
